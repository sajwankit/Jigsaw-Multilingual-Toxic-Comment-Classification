{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# I will try to use custom loss,multi-sample droput,initial weight, exploding gradient, adamw,early stopping stratified K-fold, oof, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ptfrrulabel-s2-bin/__results__.html\n",
      "/kaggle/input/ptfrrulabel-s2-bin/x_train.npy\n",
      "/kaggle/input/ptfrrulabel-s2-bin/__notebook__.ipynb\n",
      "/kaggle/input/ptfrrulabel-s2-bin/custom.css\n",
      "/kaggle/input/ptfrrulabel-s2-bin/y_train_lb_1024.npy\n",
      "/kaggle/input/ptfrrulabel-s2-bin/__output__.json\n",
      "/kaggle/input/s1-finaleseries/__results__.html\n",
      "/kaggle/input/s1-finaleseries/__notebook__.ipynb\n",
      "/kaggle/input/s1-finaleseries/s1_en.h5\n",
      "/kaggle/input/s1-finaleseries/custom.css\n",
      "/kaggle/input/s1-finaleseries/s1_en.csv\n",
      "/kaggle/input/s1-finaleseries/__output__.json\n",
      "/kaggle/input/s1-finaleseries/__results___files/__results___25_1.png\n",
      "/kaggle/input/s1-finaleseries/__results___files/__results___26_1.png\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n",
      "/kaggle/input/validtestlb/y_valid_lb_1024.npy\n",
      "/kaggle/input/validtestlb/x_test.npy\n",
      "/kaggle/input/validtestlb/x_valid.npy\n",
      "/kaggle/input/s2-finaleseries/__results__.html\n",
      "/kaggle/input/s2-finaleseries/s2_en.h5\n",
      "/kaggle/input/s2-finaleseries/__notebook__.ipynb\n",
      "/kaggle/input/s2-finaleseries/s2_en.csv\n",
      "/kaggle/input/s2-finaleseries/custom.css\n",
      "/kaggle/input/s2-finaleseries/__output__.json\n",
      "/kaggle/input/s2-finaleseries/__results___files/__results___25_1.png\n",
      "/kaggle/input/s2-finaleseries/__results___files/__results___26_1.png\n",
      "/kaggle/input/ptfrrulabel-bin/__results__.html\n",
      "/kaggle/input/ptfrrulabel-bin/x_train.npy\n",
      "/kaggle/input/ptfrrulabel-bin/__notebook__.ipynb\n",
      "/kaggle/input/ptfrrulabel-bin/custom.css\n",
      "/kaggle/input/ptfrrulabel-bin/y_train_lb_1024.npy\n",
      "/kaggle/input/ptfrrulabel-bin/__output__.json\n",
      "/kaggle/input/multi-train-714s1-label-bin/__results__.html\n",
      "/kaggle/input/multi-train-714s1-label-bin/x_train.npy\n",
      "/kaggle/input/multi-train-714s1-label-bin/__notebook__.ipynb\n",
      "/kaggle/input/multi-train-714s1-label-bin/custom.css\n",
      "/kaggle/input/multi-train-714s1-label-bin/y_train_lb_1024.npy\n",
      "/kaggle/input/multi-train-714s1-label-bin/__output__.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# HEADER IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting iterative-stratification\r\n",
      "  Downloading iterative_stratification-0.1.6-py3-none-any.whl (8.7 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification) (1.18.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification) (0.23.1)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification) (1.4.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification) (0.14.1)\r\n",
      "Installing collected packages: iterative-stratification\r\n",
      "Successfully installed iterative-stratification-0.1.6\r\n"
     ]
    }
   ],
   "source": [
    "!pip install iterative-stratification\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (2.11.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.91)\r\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.7.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.1)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.2)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.24.3)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gin-config==0.1.1\r\n",
      "  Downloading gin-config-0.1.1.tar.gz (40 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 40 kB 1.7 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from gin-config==0.1.1) (1.14.0)\r\n",
      "Building wheels for collected packages: gin-config\r\n",
      "  Building wheel for gin-config (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for gin-config: filename=gin_config-0.1.1-py3-none-any.whl size=38329 sha256=bb4c4ac8743252d841efbca35590a68d8ab90d24162804612e19ec18d666188b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/26/b3/1b/b745a6f67881260cf669db1d4fc55d0c961e1e0d6317fdf606\r\n",
      "Successfully built gin-config\r\n",
      "Installing collected packages: gin-config\r\n",
      "Successfully installed gin-config-0.1.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gin-config==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
    "from tensorflow.keras import layers as L\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    if epoch <=1:\n",
    "        return 0.000007\n",
    "    else:\n",
    "        return 0.000007 /(epoch * 0.75)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('/kaggle/input/ptfrrulabel-bin/x_train.npy')\n",
    "x_valid = np.load('/kaggle/input/validtestlb/x_valid.npy')\n",
    "x_test = np.load('/kaggle/input/validtestlb/x_test.npy')\n",
    "\n",
    "y_valid = np.load('/kaggle/input/validtestlb/y_valid_lb_1024.npy')\n",
    "y_train = np.load('/kaggle/input/ptfrrulabel-bin/y_train_lb_1024.npy')\n",
    "import sklearn\n",
    "x_train, y_train = sklearn.utils.shuffle(x_train, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 256  #Reduced for quicker execution\n",
    "LR = 0.000007\n",
    "BATCH_SIZE = 16 # per TPU core\n",
    "TOTAL_STEPS_STAGE1 = len(x_train)//(16*8)\n",
    "VALIDATE_EVERY_STAGE1 = 100\n",
    "TOTAL_STEPS_STAGE2 = 200\n",
    "VALIDATE_EVERY_STAGE2 = 100\n",
    "label_bins = 1024\n",
    "\n",
    "PRETRAINED_MODEL = 'jplu/tf-xlm-roberta-large'\n",
    "D = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import logging\n",
    "# no extensive logging \n",
    "logging.getLogger().setLevel(logging.NOTSET)\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "def connect_to_TPU():\n",
    "    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n",
    "    try:\n",
    "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "        # set: this is always the case on Kaggle.\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    else:\n",
    "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "    return tpu, strategy, global_batch_size\n",
    "\n",
    "\n",
    "tpu, strategy, global_batch_size = connect_to_TPU()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_dataset(X, y=None, training=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "    ### Add y if present ###\n",
    "    if y is not None:\n",
    "        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n",
    "        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n",
    "        \n",
    "#     ### Repeat if training ###\n",
    "#     if training:\n",
    "#         dataset = dataset.shuffle(len(X)).repeat()\n",
    "\n",
    "    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n",
    "\n",
    "    ### make it distributed  ###\n",
    "    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    return dist_dataset\n",
    "    \n",
    "    \n",
    "train_dist_dataset = create_dist_dataset(x_train, y_train, True)\n",
    "val_dist_dataset   = create_dist_dataset(x_valid)\n",
    "test_dist_dataset  = create_dist_dataset(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 256, 1024),  559890432   input_word_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10240)        10496000    tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, 10240)        0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5120)         52433920    dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)            (None, 5120)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)            (None, 5120)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)            (None, 5120)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)            (None, 5120)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)            (None, 5120)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         5243904     dropout_75[0][0]                 \n",
      "                                                                 dropout_76[0][0]                 \n",
      "                                                                 dropout_77[0][0]                 \n",
      "                                                                 dropout_78[0][0]                 \n",
      "                                                                 dropout_79[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "average (Average)               (None, 1024)         0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "==================================================================================================\n",
      "Total params: 628,064,256\n",
      "Trainable params: 628,064,256\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "CPU times: user 2min 8s, sys: 45.5 s, total: 2min 53s\n",
      "Wall time: 2min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def create_model_and_optimizer():\n",
    "    with strategy.scope():\n",
    "        transformer_layer = TFAutoModel.from_pretrained(PRETRAINED_MODEL)                \n",
    "        model = build_model(transformer_layer)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=LR, epsilon=1e-06)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def build_model(transformer):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "    \"\"\"\n",
    "    input_word_ids = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    out1 = tf.keras.layers.Dense(10240, activation='relu', kernel_initializer=tf.initializers.he_normal())(cls_token)\n",
    "    out2 = tf.keras.layers.Dropout(0.35)(out1)\n",
    "    out3 = tf.keras.layers.Dense(5120, activation='relu', kernel_initializer=tf.initializers.he_normal())(out2)\n",
    "    FC = tf.keras.layers.Dense(label_bins, activation='sigmoid')\n",
    "    fcdropouts = []\n",
    "    for i in range(5):\n",
    "        x = tf.keras.layers.Dropout(0.5)(out3)\n",
    "        x = FC(x)\n",
    "        fcdropouts.append(x)\n",
    "        \n",
    "    out = tf.keras.layers.Average()(fcdropouts)\n",
    "\n",
    "    model = Model(inputs=[input_word_ids], outputs=[out])\n",
    "#     model.compile(optimizer=Adam(0.000007), loss ='binary_crossentropy',metrics=[precision_m, f1_m, recall_m])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model, optimizer = create_model_and_optimizer()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/kaggle/input/s1-finaleseries/s1_en.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_tf(y_true, y_pred):\n",
    "    recall = 0\n",
    "    precision = 0\n",
    "    f1=0\n",
    "#     if (K.int_shape(y_pred))[1]== label_bins and (K.int_shape(y_true))[1]== label_bins:\n",
    "    y_true = K.cast(K.greater_equal(K.cast(K.mean(y_true,1),'float32'), K.constant(0.5)),'int32')\n",
    "    y_pred = K.cast(K.greater_equal(K.cast(K.mean(y_pred,1),'float32'), K.constant(0.5)),'int32')\n",
    "    true_positives = K.cast(K.sum(y_true * y_pred),'float32')\n",
    "    possible_positives = K.cast(K.sum(y_true),'float32')\n",
    "    predicted_positives = K.cast(K.sum(y_pred),'float32')\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    f1=2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "    return recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y_true, y_pred):\n",
    "    recall = 0\n",
    "    precision = 0\n",
    "    f1=0\n",
    "    y_true = (np.sum(y_true,axis=1)/label_bins)        \n",
    "    y_true[y_true >= .5] = 1\n",
    "    y_true[y_true < .5] = 0\n",
    "    y_pred = (np.sum(y_pred,axis=1)/label_bins)    \n",
    "    y_pred[y_pred >= .5] = 1\n",
    "    y_pred[y_pred < .5] = 0\n",
    "    true_positives = int(np.sum(y_true * y_pred))\n",
    "    possible_positives = int(np.sum(y_true))\n",
    "    predicted_positives = int(np.sum(y_pred))\n",
    "    recall = true_positives / (possible_positives + 1e-07)\n",
    "    precision = true_positives / (predicted_positives + 1e-07)\n",
    "    f1=2*((precision*recall)/(precision+recall+1e-07))\n",
    "    return recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_tef(labels, predictions):\n",
    "    recall,precision,f1=metric_tf(labels, predictions)\n",
    "    return recall,precision,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_losses_and_metrics():\n",
    "    with strategy.scope():\n",
    "        recall,precision,f1=0,0,0\n",
    "        loss_object = tf.keras.losses.BinaryCrossentropy(\n",
    "            reduction=tf.keras.losses.Reduction.NONE, from_logits=False)\n",
    "        def compute_loss(labels, predictions):\n",
    "            per_example_loss = loss_object(labels, predictions)\n",
    "            loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss, global_batch_size = global_batch_size)\n",
    "            return loss\n",
    "        def metrics_tef(labels, predictions):\n",
    "            recall,precision,f1=metric_tf(labels, predictions)\n",
    "        train_accuracy_metric = tf.keras.metrics.AUC(name='training_AUC')\n",
    "        \n",
    "    return compute_loss, train_accuracy_metric, recall,precision,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dist_dataset, val_dist_dataset=None, y_val=None,\n",
    "          total_steps=5000, validate_every=500):\n",
    "    best_weights, history = None, []\n",
    "    step = 0\n",
    "    total_loss = 0.0\n",
    "    total_recall = 0.0\n",
    "    total_precision = 0.0\n",
    "    total_f1 = 0.0\n",
    "    ### Training lopp ###\n",
    "    for tensor in train_dist_dataset:\n",
    "#         total_loss=total_loss+distributed_train_step(tensor)\n",
    "        loss, recall,precision,f1 = distributed_train_step(tensor)\n",
    "        total_loss=loss+total_loss\n",
    "        total_recall=recall+total_recall\n",
    "        total_precision=precision+total_precision\n",
    "        total_f1=f1+total_f1\n",
    "        \n",
    "        step+=1\n",
    "\n",
    "        \n",
    "        if (step % validate_every == 0):\n",
    "                ### Print train metrics ###  \n",
    "                \n",
    "            train_loss = total_loss / step\n",
    "            train_recall = total_recall / step\n",
    "            train_precision = total_precision / step\n",
    "            train_f1 = total_f1 / step    \n",
    "            train_metric = train_accuracy_metric.result().numpy()\n",
    "\n",
    "#             print(recall.eval(session=tf.compat.v1.Session()))\n",
    "            print(\"Step %d, train AUC: %.5f\" % (step, train_metric))\n",
    "            print(\"Step %d, train_loss: %.5f\" % (step, train_loss))\n",
    "            print(\"Step %d, train_recall: %.5f\" % (step, train_recall))\n",
    "            print(\"Step %d, total_precision: %.5f\" % (step, train_precision))\n",
    "            print(\"Step %d, total_f1: %.5f\" % (step, train_f1))\n",
    "#             pre=tf.keras.backend.eval(loss)\n",
    "#             print(pre)\n",
    "            ### Test loop with exact AUC ###\n",
    "    #         del y_pred_train\n",
    "            if val_dist_dataset:\n",
    "                y_pred_val = predict(val_dist_dataset)\n",
    "                recall_v,precision_v,f1_v=metric(y_valid,y_pred_val)\n",
    "                x=(np.sum(y_valid, axis = 1)/label_bins).astype('int32')\n",
    "                x[x >= .5 ] = 1\n",
    "                x[x < .5 ] = 0\n",
    "                val_metric = roc_auc_score(x, (np.sum(y_pred_val, axis = 1)/label_bins))\n",
    "                print(\"     validation AUC: %.5f\" %  val_metric)\n",
    "                print(\"     validation precision: %.5f\" %  precision_v)\n",
    "                print(\"     validation recall: %.5f\" %  recall_v)        \n",
    "                print(\"     validation f1: %.5f\" %  f1_v)\n",
    "                \n",
    "#                 # save weights if it is the best yet\n",
    "#                 history.append(val_metric)\n",
    "#                 if history[-1] == max(history):\n",
    "#                     best_weights = model.get_weights()\n",
    "                    \n",
    "                    \n",
    "    #             del y_pred_val\n",
    "            ### Reset (train) metrics ###\n",
    "            train_accuracy_metric.reset_states()\n",
    "            \n",
    "        if step  == total_steps:\n",
    "            break\n",
    "#     model.set_weights(best_weights)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(data):\n",
    "    per_replica_losses, p_recall,p_precision,p_f1 = strategy.run(train_step, args=(data,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
    "                         axis=None), strategy.reduce(tf.distribute.ReduceOp.MEAN, p_recall,\n",
    "                         axis=None),strategy.reduce(tf.distribute.ReduceOp.MEAN, p_precision,\n",
    "                         axis=None),strategy.reduce(tf.distribute.ReduceOp.MEAN, p_f1,\n",
    "                         axis=None)\n",
    "\n",
    "def train_step(inputs):\n",
    "    features, labels = inputs\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(features, training=True)\n",
    "        loss = compute_loss(labels, predictions)\n",
    "        recall,precision,f1=metrics_tef(labels, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_accuracy_metric.update_state(labels, predictions)\n",
    "    return loss, recall,precision,f1\n",
    "#     tf.print(precision)\n",
    "#     print(\"      recall: %.5f\" %  recall.numpy())\n",
    "#     print(\"      f1: %.5f\" %  f1.numpy())\n",
    "#     train_accuracy_metric.update_state(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset):  \n",
    "    predictions = []\n",
    "    for tensor in dataset:\n",
    "        predictions.append(distributed_prediction_step(tensor))\n",
    "    ### stack replicas and batches\n",
    "    predictions = np.vstack(list(map(np.vstack,predictions)))\n",
    "    return predictions\n",
    "\n",
    "@tf.function\n",
    "def distributed_prediction_step(data):\n",
    "    predictions = strategy.experimental_run_v2(prediction_step, args=(data,))\n",
    "    return strategy.experimental_local_results(predictions)\n",
    "\n",
    "def prediction_step(inputs):\n",
    "    features = inputs  # note datasets used in prediction do not have labels\n",
    "    predictions = model(features, training=False)\n",
    "    return predictions\n",
    "\n",
    "compute_loss, train_accuracy_metric, recall, precision, f1 = define_losses_and_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, train AUC: 0.94913\n",
      "Step 100, train_loss: 0.28533\n",
      "Step 100, train_recall: 0.60871\n",
      "Step 100, total_precision: 0.14800\n",
      "Step 100, total_f1: 0.22842\n",
      "     validation AUC: 0.93122\n",
      "     validation precision: 0.53536\n",
      "     validation recall: 0.80000\n",
      "     validation f1: 0.64146\n",
      "Step 200, train AUC: 0.95236\n",
      "Step 200, train_loss: 0.28078\n",
      "Step 200, train_recall: 0.61429\n",
      "Step 200, total_precision: 0.15062\n",
      "Step 200, total_f1: 0.23200\n",
      "     validation AUC: 0.93505\n",
      "     validation precision: 0.54247\n",
      "     validation recall: 0.82033\n",
      "     validation f1: 0.65307\n",
      "Step 300, train AUC: 0.95508\n",
      "Step 300, train_loss: 0.27654\n",
      "Step 300, train_recall: 0.61723\n",
      "Step 300, total_precision: 0.15029\n",
      "Step 300, total_f1: 0.23180\n",
      "     validation AUC: 0.93507\n",
      "     validation precision: 0.54037\n",
      "     validation recall: 0.83252\n",
      "     validation f1: 0.65536\n",
      "Step 400, train AUC: 0.95532\n",
      "Step 400, train_loss: 0.27422\n",
      "Step 400, train_recall: 0.61402\n",
      "Step 400, total_precision: 0.15052\n",
      "Step 400, total_f1: 0.23144\n",
      "     validation AUC: 0.93213\n",
      "     validation precision: 0.57548\n",
      "     validation recall: 0.72520\n",
      "     validation f1: 0.64173\n",
      "Step 500, train AUC: 0.95435\n",
      "Step 500, train_loss: 0.27340\n",
      "Step 500, train_recall: 0.61810\n",
      "Step 500, total_precision: 0.15132\n",
      "Step 500, total_f1: 0.23279\n",
      "     validation AUC: 0.93353\n",
      "     validation precision: 0.54410\n",
      "     validation recall: 0.80244\n",
      "     validation f1: 0.64849\n",
      "Step 600, train AUC: 0.95648\n",
      "Step 600, train_loss: 0.27187\n",
      "Step 600, train_recall: 0.62371\n",
      "Step 600, total_precision: 0.15227\n",
      "Step 600, total_f1: 0.23435\n",
      "     validation AUC: 0.93120\n",
      "     validation precision: 0.54673\n",
      "     validation recall: 0.78943\n",
      "     validation f1: 0.64604\n",
      "Step 700, train AUC: 0.95845\n",
      "Step 700, train_loss: 0.26993\n",
      "Step 700, train_recall: 0.62790\n",
      "Step 700, total_precision: 0.15318\n",
      "Step 700, total_f1: 0.23573\n",
      "     validation AUC: 0.93374\n",
      "     validation precision: 0.56576\n",
      "     validation recall: 0.77642\n",
      "     validation f1: 0.65456\n",
      "Step 800, train AUC: 0.95738\n",
      "Step 800, train_loss: 0.26886\n",
      "Step 800, train_recall: 0.63169\n",
      "Step 800, total_precision: 0.15454\n",
      "Step 800, total_f1: 0.23763\n",
      "     validation AUC: 0.93207\n",
      "     validation precision: 0.57468\n",
      "     validation recall: 0.73821\n",
      "     validation f1: 0.64626\n",
      "Step 900, train AUC: 0.95821\n",
      "Step 900, train_loss: 0.26771\n",
      "Step 900, train_recall: 0.63150\n",
      "Step 900, total_precision: 0.15449\n",
      "Step 900, total_f1: 0.23764\n",
      "     validation AUC: 0.93349\n",
      "     validation precision: 0.54595\n",
      "     validation recall: 0.80650\n",
      "     validation f1: 0.65113\n",
      "Step 1000, train AUC: 0.95808\n",
      "Step 1000, train_loss: 0.26677\n",
      "Step 1000, train_recall: 0.63267\n",
      "Step 1000, total_precision: 0.15501\n",
      "Step 1000, total_f1: 0.23845\n",
      "     validation AUC: 0.93152\n",
      "     validation precision: 0.54333\n",
      "     validation recall: 0.79512\n",
      "     validation f1: 0.64554\n",
      "Step 1100, train AUC: 0.95902\n",
      "Step 1100, train_loss: 0.26576\n",
      "Step 1100, train_recall: 0.63496\n",
      "Step 1100, total_precision: 0.15573\n",
      "Step 1100, total_f1: 0.23947\n",
      "     validation AUC: 0.92366\n",
      "     validation precision: 0.60803\n",
      "     validation recall: 0.61545\n",
      "     validation f1: 0.61172\n",
      "Step 1200, train AUC: 0.95804\n",
      "Step 1200, train_loss: 0.26518\n",
      "Step 1200, train_recall: 0.63446\n",
      "Step 1200, total_precision: 0.15564\n",
      "Step 1200, total_f1: 0.23938\n",
      "     validation AUC: 0.93017\n",
      "     validation precision: 0.57743\n",
      "     validation recall: 0.71545\n",
      "     validation f1: 0.63907\n",
      "Step 1300, train AUC: 0.95855\n",
      "Step 1300, train_loss: 0.26459\n",
      "Step 1300, train_recall: 0.63512\n",
      "Step 1300, total_precision: 0.15588\n",
      "Step 1300, total_f1: 0.23967\n",
      "     validation AUC: 0.93050\n",
      "     validation precision: 0.59241\n",
      "     validation recall: 0.67236\n",
      "     validation f1: 0.62986\n",
      "Step 1400, train AUC: 0.95825\n",
      "Step 1400, train_loss: 0.26413\n",
      "Step 1400, train_recall: 0.63738\n",
      "Step 1400, total_precision: 0.15616\n",
      "Step 1400, total_f1: 0.24024\n",
      "     validation AUC: 0.93139\n",
      "     validation precision: 0.56485\n",
      "     validation recall: 0.75772\n",
      "     validation f1: 0.64722\n",
      "Step 1500, train AUC: 0.96070\n",
      "Step 1500, train_loss: 0.26317\n",
      "Step 1500, train_recall: 0.63817\n",
      "Step 1500, total_precision: 0.15643\n",
      "Step 1500, total_f1: 0.24058\n",
      "     validation AUC: 0.93212\n",
      "     validation precision: 0.54931\n",
      "     validation recall: 0.77886\n",
      "     validation f1: 0.64425\n",
      "Step 1600, train AUC: 0.95905\n",
      "Step 1600, train_loss: 0.26269\n",
      "Step 1600, train_recall: 0.63933\n",
      "Step 1600, total_precision: 0.15669\n",
      "Step 1600, total_f1: 0.24099\n",
      "     validation AUC: 0.92774\n",
      "     validation precision: 0.58877\n",
      "     validation recall: 0.68211\n",
      "     validation f1: 0.63202\n",
      "Step 1700, train AUC: 0.95898\n",
      "Step 1700, train_loss: 0.26231\n",
      "Step 1700, train_recall: 0.64067\n",
      "Step 1700, total_precision: 0.15684\n",
      "Step 1700, total_f1: 0.24136\n",
      "     validation AUC: 0.93032\n",
      "     validation precision: 0.55542\n",
      "     validation recall: 0.76585\n",
      "     validation f1: 0.64388\n",
      "Step 1800, train AUC: 0.96010\n",
      "Step 1800, train_loss: 0.26177\n",
      "Step 1800, train_recall: 0.64101\n",
      "Step 1800, total_precision: 0.15662\n",
      "Step 1800, total_f1: 0.24113\n",
      "     validation AUC: 0.93278\n",
      "     validation precision: 0.59061\n",
      "     validation recall: 0.72602\n",
      "     validation f1: 0.65135\n",
      "Step 1900, train AUC: 0.96056\n",
      "Step 1900, train_loss: 0.26123\n",
      "Step 1900, train_recall: 0.64020\n",
      "Step 1900, total_precision: 0.15630\n",
      "Step 1900, total_f1: 0.24072\n",
      "     validation AUC: 0.93067\n",
      "     validation precision: 0.61725\n",
      "     validation recall: 0.66341\n",
      "     validation f1: 0.63950\n",
      "Step 2000, train AUC: 0.95975\n",
      "Step 2000, train_loss: 0.26086\n",
      "Step 2000, train_recall: 0.63986\n",
      "Step 2000, total_precision: 0.15605\n",
      "Step 2000, total_f1: 0.24040\n",
      "     validation AUC: 0.93119\n",
      "     validation precision: 0.55086\n",
      "     validation recall: 0.78374\n",
      "     validation f1: 0.64698\n",
      "Step 2100, train AUC: 0.96144\n",
      "Step 2100, train_loss: 0.26027\n",
      "Step 2100, train_recall: 0.64042\n",
      "Step 2100, total_precision: 0.15602\n",
      "Step 2100, total_f1: 0.24044\n",
      "     validation AUC: 0.92860\n",
      "     validation precision: 0.54001\n",
      "     validation recall: 0.78455\n",
      "     validation f1: 0.63971\n",
      "Step 2200, train AUC: 0.96006\n",
      "Step 2200, train_loss: 0.25994\n",
      "Step 2200, train_recall: 0.64089\n",
      "Step 2200, total_precision: 0.15570\n",
      "Step 2200, total_f1: 0.24010\n",
      "     validation AUC: 0.92982\n",
      "     validation precision: 0.55776\n",
      "     validation recall: 0.75366\n",
      "     validation f1: 0.64108\n",
      "Step 2300, train AUC: 0.96195\n",
      "Step 2300, train_loss: 0.25937\n",
      "Step 2300, train_recall: 0.64053\n",
      "Step 2300, total_precision: 0.15529\n",
      "Step 2300, total_f1: 0.23958\n",
      "     validation AUC: 0.92757\n",
      "     validation precision: 0.53930\n",
      "     validation recall: 0.76423\n",
      "     validation f1: 0.63236\n",
      "Step 2400, train AUC: 0.95944\n",
      "Step 2400, train_loss: 0.25920\n",
      "Step 2400, train_recall: 0.64143\n",
      "Step 2400, total_precision: 0.15540\n",
      "Step 2400, total_f1: 0.23979\n",
      "     validation AUC: 0.92854\n",
      "     validation precision: 0.56783\n",
      "     validation recall: 0.71463\n",
      "     validation f1: 0.63283\n",
      "Step 2500, train AUC: 0.96107\n",
      "Step 2500, train_loss: 0.25882\n",
      "Step 2500, train_recall: 0.64153\n",
      "Step 2500, total_precision: 0.15546\n",
      "Step 2500, total_f1: 0.23987\n",
      "     validation AUC: 0.93141\n",
      "     validation precision: 0.57324\n",
      "     validation recall: 0.73496\n",
      "     validation f1: 0.64410\n",
      "CPU times: user 4min 45s, sys: 49 s, total: 5min 34s\n",
      "Wall time: 28min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dist_dataset, val_dist_dataset, y_valid,\n",
    "      TOTAL_STEPS_STAGE1, VALIDATE_EVERY_STAGE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # make a new dataset for training with the validation data \n",
    "# # with targets, shuffling and repeating\n",
    "# val_dist_dataset_4_training = create_dist_dataset(x_valid, y_valid, training=True)\n",
    "\n",
    "# # train again\n",
    "# train(val_dist_dataset_4_training,\n",
    "#       total_steps = TOTAL_STEPS_STAGE2, \n",
    "#       validate_every = VALIDATE_EVERY_STAGE2)  # not validating but printing now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('s1_en_with_multi3l.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = predict(test_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "for a in (x):\n",
    "    X.append(float(sum(a)/len(a)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\n",
    "test['toxic'] = np.array(X)\n",
    "sub = test[['id', 'toxic']]\n",
    "sub.to_csv('s1_en_multi3l.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4ca25ab750>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub['toxic'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4cd81cd850>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAWv0lEQVR4nO3db4yd5Znf8e8vOGHdZCEEwsiy2ZoWtw1/GrJMvVbTVrNhVRz2BUQCySmK2Q2VU0qqrOQXC3nRZBVZCi9YKmhg5SwRBrEBiyQ13Q3bItjTdLX8WbMiMYbQTIMLDhaIwBImFTTjXH1xbreHYTxz5szxHI/n+5GO5jnX89znua/Bmt95/pxDqgpJkt4z6glIko4PBoIkCTAQJEmNgSBJAgwESVKzatQTGNQZZ5xR69evH2jsz3/+c97//vcPd0LHOXteGex5ZVhMz08++eSrVfXh2dYt20BYv349e/fuHWhsp9NhYmJiuBM6ztnzymDPK8Niek7yv462zlNGkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJGAZf1J5Mfb95A1+5/o/G8m+D3z1t0eyX0maj0cIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEnNvIGQ5FeSPJHk+0n2J/mDVv9ykp8keao9Lu0Zc0OSySTPJbmkp35Rkn1t3S1J0uonJ7mv1R9Psn74rUqS5tLPEcLbwCeq6qPAhcDmJJvaupur6sL2+C5AknOBLcB5wGbgtiQnte1vB7YBG9pjc6tfA7xeVecANwM3Lr41SdJCzBsI1TXVnr63PWqOIZcB91bV21X1PDAJbEyyBjilqh6tqgLuAi7vGbOrLd8PXHzk6EGStDT6+v8htHf4TwLnAF+rqseTfBL4fJKtwF5ge1W9DqwFHusZfrDVftGWZ9ZpP18EqKrpJG8ApwOvzpjHNrpHGIyNjdHpdPrvtMfYath+wfRAYxdr0Dkv1tTU1Mj2PSr2vDLY8/D0FQhVdRi4MMkHge8kOZ/u6Z+v0D1a+ApwE/BZYLZ39jVHnXnW9c5jJ7ATYHx8vCYmJvqZ/rvces8ebto3mv830IGrJkay306nw6C/r+XKnlcGex6eBd1lVFV/C3SAzVX1clUdrqpfAl8HNrbNDgJn9QxbB7zU6utmqb9jTJJVwKnAawvqRJK0KP3cZfThdmRAktXAbwE/bNcEjvgU8HRbfgDY0u4cOpvuxeMnquoQ8GaSTe36wFZgT8+Yq9vyFcAj7TqDJGmJ9HPeZA2wq11HeA+wu6r+NMndSS6ke2rnAPA5gKran2Q38AwwDVzXTjkBXAvcCawGHmwPgDuAu5NM0j0y2DKE3iRJCzBvIFTVD4CPzVL/zBxjdgA7ZqnvBc6fpf4WcOV8c5EkHTt+UlmSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkZt5ASPIrSZ5I8v0k+5P8Qat/KMlDSX7Ufp7WM+aGJJNJnktySU/9oiT72rpbkqTVT05yX6s/nmT98FuVJM2lnyOEt4FPVNVHgQuBzUk2AdcDD1fVBuDh9pwk5wJbgPOAzcBtSU5qr3U7sA3Y0B6bW/0a4PWqOge4GbhxCL1JkhZg3kCorqn29L3tUcBlwK5W3wVc3pYvA+6tqrer6nlgEtiYZA1wSlU9WlUF3DVjzJHXuh+4+MjRgyRpaazqZ6P2Dv9J4Bzga1X1eJKxqjoEUFWHkpzZNl8LPNYz/GCr/aItz6wfGfNie63pJG8ApwOvzpjHNrpHGIyNjdHpdPps853GVsP2C6YHGrtYg855saampka271Gx55XBnoenr0CoqsPAhUk+CHwnyflzbD7bO/uaoz7XmJnz2AnsBBgfH6+JiYm5pn1Ut96zh5v29dX60B24amIk++10Ogz6+1qu7HllsOfhWdBdRlX1t0CH7rn/l9tpINrPV9pmB4GzeoatA15q9XWz1N8xJskq4FTgtYXMTZK0OP3cZfThdmRAktXAbwE/BB4Arm6bXQ3sacsPAFvanUNn0714/EQ7vfRmkk3t+sDWGWOOvNYVwCPtOoMkaYn0c95kDbCrXUd4D7C7qv40yaPA7iTXAC8AVwJU1f4ku4FngGngunbKCeBa4E5gNfBgewDcAdydZJLukcGWYTQnSerfvIFQVT8APjZL/afAxUcZswPYMUt9L/Cu6w9V9RYtUCRJo+EnlSVJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElq5g2EJGcl+YskzybZn+QLrf7lJD9J8lR7XNoz5oYkk0meS3JJT/2iJPvauluSpNVPTnJfqz+eZP3wW5UkzaWfI4RpYHtVfQTYBFyX5Ny27uaqurA9vgvQ1m0BzgM2A7clOaltfzuwDdjQHptb/Rrg9ao6B7gZuHHxrUmSFmLeQKiqQ1X1N235TeBZYO0cQy4D7q2qt6vqeWAS2JhkDXBKVT1aVQXcBVzeM2ZXW74fuPjI0YMkaWmsWsjG7VTOx4DHgY8Dn0+yFdhL9yjidbph8VjPsIOt9ou2PLNO+/kiQFVNJ3kDOB14dcb+t9E9wmBsbIxOp7OQ6f8/Y6th+wXTA41drEHnvFhTU1Mj2/eo2PPKYM/D03cgJPkA8C3g96rqZ0luB74CVPt5E/BZYLZ39jVHnXnW/f9C1U5gJ8D4+HhNTEz0O/13uPWePdy0b0FZODQHrpoYyX47nQ6D/r6WK3teGex5ePq6yyjJe+mGwT1V9W2Aqnq5qg5X1S+BrwMb2+YHgbN6hq8DXmr1dbPU3zEmySrgVOC1QRqSJA2mn7uMAtwBPFtVf9hTX9Oz2aeAp9vyA8CWdufQ2XQvHj9RVYeAN5Nsaq+5FdjTM+bqtnwF8Ei7ziBJWiL9nDf5OPAZYF+Sp1rti8Cnk1xI99TOAeBzAFW1P8lu4Bm6dyhdV1WH27hrgTuB1cCD7QHdwLk7ySTdI4Mti2tLkrRQ8wZCVf0ls5/j/+4cY3YAO2ap7wXOn6X+FnDlfHORJB07flJZkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpGbeQEhyVpK/SPJskv1JvtDqH0ryUJIftZ+n9Yy5IclkkueSXNJTvyjJvrbuliRp9ZOT3NfqjydZP/xWJUlz6ecIYRrYXlUfATYB1yU5F7geeLiqNgAPt+e0dVuA84DNwG1JTmqvdTuwDdjQHptb/Rrg9ao6B7gZuHEIvUmSFmDeQKiqQ1X1N235TeBZYC1wGbCrbbYLuLwtXwbcW1VvV9XzwCSwMcka4JSqerSqCrhrxpgjr3U/cPGRowdJ0tJY0DWEdirnY8DjwFhVHYJuaABnts3WAi/2DDvYamvb8sz6O8ZU1TTwBnD6QuYmSVqcVf1umOQDwLeA36uqn83xBn62FTVHfa4xM+ewje4pJ8bGxuh0OvPMenZjq2H7BdMDjV2sQee8WFNTUyPb96jY88pgz8PTVyAkeS/dMLinqr7dyi8nWVNVh9rpoFda/SBwVs/wdcBLrb5ulnrvmINJVgGnAq/NnEdV7QR2AoyPj9fExEQ/03+XW+/Zw037+s7CoTpw1cRI9tvpdBj097Vc2fPKYM/D089dRgHuAJ6tqj/sWfUAcHVbvhrY01Pf0u4cOpvuxeMn2mmlN5Nsaq+5dcaYI691BfBIu84gSVoi/bxN/jjwGWBfkqda7YvAV4HdSa4BXgCuBKiq/Ul2A8/QvUPpuqo63MZdC9wJrAYebA/oBs7dSSbpHhlsWWRfkqQFmjcQquovmf0cP8DFRxmzA9gxS30vcP4s9bdogSJJGg0/qSxJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkoA+AiHJN5K8kuTpntqXk/wkyVPtcWnPuhuSTCZ5LsklPfWLkuxr625JklY/Ocl9rf54kvXDbVGS1I9+jhDuBDbPUr+5qi5sj+8CJDkX2AKc18bcluSktv3twDZgQ3scec1rgNer6hzgZuDGAXuRJC3CvIFQVd8DXuvz9S4D7q2qt6vqeWAS2JhkDXBKVT1aVQXcBVzeM2ZXW74fuPjI0YMkaemsWsTYzyfZCuwFtlfV68Ba4LGebQ622i/a8sw67eeLAFU1neQN4HTg1Zk7TLKN7lEGY2NjdDqdgSY+thq2XzA90NjFGnTOizU1NTWyfY+KPa8M9jw8gwbC7cBXgGo/bwI+C8z2zr7mqDPPuncWq3YCOwHGx8drYmJiQZM+4tZ79nDTvsVk4eAOXDUxkv12Oh0G/X0tV/a8Mtjz8Ax0l1FVvVxVh6vql8DXgY1t1UHgrJ5N1wEvtfq6WervGJNkFXAq/Z+ikiQNyUCB0K4JHPEp4MgdSA8AW9qdQ2fTvXj8RFUdAt5MsqldH9gK7OkZc3VbvgJ4pF1nkCQtoXnPmyT5JjABnJHkIPAlYCLJhXRP7RwAPgdQVfuT7AaeAaaB66rqcHupa+nesbQaeLA9AO4A7k4ySffIYMswGpMkLcy8gVBVn56lfMcc2+8AdsxS3wucP0v9LeDK+eYhSTq2/KSyJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSc28gZDkG0leSfJ0T+1DSR5K8qP287SedTckmUzyXJJLeuoXJdnX1t2SJK1+cpL7Wv3xJOuH26IkqR/9HCHcCWyeUbseeLiqNgAPt+ckORfYApzXxtyW5KQ25nZgG7ChPY685jXA61V1DnAzcOOgzUiSBjdvIFTV94DXZpQvA3a15V3A5T31e6vq7ap6HpgENiZZA5xSVY9WVQF3zRhz5LXuBy4+cvQgSVo6qwYcN1ZVhwCq6lCSM1t9LfBYz3YHW+0XbXlm/ciYF9trTSd5AzgdeHXmTpNso3uUwdjYGJ1OZ7DJr4btF0wPNHaxBp3zYk1NTY1s36NizyuDPQ/PoIFwNLO9s6856nONeXexaiewE2B8fLwmJiYGmCLces8ebto37Nb7c+CqiZHst9PpMOjva7my55XBnodn0LuMXm6ngWg/X2n1g8BZPdutA15q9XWz1N8xJskq4FTefYpKknSMDRoIDwBXt+WrgT099S3tzqGz6V48fqKdXnozyaZ2fWDrjDFHXusK4JF2nUGStITmPW+S5JvABHBGkoPAl4CvAruTXAO8AFwJUFX7k+wGngGmgeuq6nB7qWvp3rG0GniwPQDuAO5OMkn3yGDLUDqTJC3IvIFQVZ8+yqqLj7L9DmDHLPW9wPmz1N+iBYokaXT8pLIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJzaICIcmBJPuSPJVkb6t9KMlDSX7Ufp7Ws/0NSSaTPJfkkp76Re11JpPckiSLmZckaeGGcYTwm1V1YVWNt+fXAw9X1Qbg4facJOcCW4DzgM3AbUlOamNuB7YBG9pj8xDmJUlagGNxyugyYFdb3gVc3lO/t6rerqrngUlgY5I1wClV9WhVFXBXzxhJ0hJZbCAU8F+TPJlkW6uNVdUhgPbzzFZfC7zYM/Zgq61tyzPrkqQltGqR4z9eVS8lORN4KMkP59h2tusCNUf93S/QDZ1tAGNjY3Q6nQVOt2tsNWy/YHqgsYs16JwXa2pqamT7HhV7XhnseXgWFQhV9VL7+UqS7wAbgZeTrKmqQ+100Ctt84PAWT3D1wEvtfq6Weqz7W8nsBNgfHy8JiYmBpr3rffs4aZ9i83CwRy4amIk++10Ogz6+1qu7HllsOfhGfiUUZL3J/nVI8vAvwSeBh4Arm6bXQ3sacsPAFuSnJzkbLoXj59op5XeTLKp3V20tWeMJGmJLOZt8hjwnXaH6CrgT6rqz5P8NbA7yTXAC8CVAFW1P8lu4BlgGriuqg6317oWuBNYDTzYHpKkJTRwIFTVj4GPzlL/KXDxUcbsAHbMUt8LnD/oXCRJi+cnlSVJgIEgSWpGc6uNdAJbf/2fjWzfd25+/8j2reXPQNAJa99P3uB3RvjHWSe2EzH4PWUkSQIMBElS4ymjJTaqw8xRnlseVc/bLxjJbkdqVKfJDnz1t5d8nxo+A2GF8Hy6pPl4ykiSBBgIkqTGU0aSFu1EvAVzJfIIQZIEeIQgaZnzhonh8QhBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqjptASLI5yXNJJpNcP+r5SNJKc1wEQpKTgK8BnwTOBT6d5NzRzkqSVpbjIhCAjcBkVf24qv4PcC9w2YjnJEkrSqpq1HMgyRXA5qr61+35Z4DfqKrPz9huG7CtPf2HwHMD7vIM4NUBxy5X9rwy2PPKsJie/25VfXi2FcfLV1dkltq7kqqqdgI7F72zZG9VjS/2dZYTe14Z7HllOFY9Hy+njA4CZ/U8Xwe8NKK5SNKKdLwEwl8DG5KcneR9wBbggRHPSZJWlOPilFFVTSf5PPBfgJOAb1TV/mO4y0WfdlqG7HllsOeV4Zj0fFxcVJYkjd7xcspIkjRiBoIkCTjBA2G+r8NI1y1t/Q+S/Poo5jlMffR8Vev1B0n+KslHRzHPYen3K0+S/JMkh9tnXpa1fnpOMpHkqST7k/y3pZ7jsPXx7/rUJP85yfdbz787inkOU5JvJHklydNHWT/8v19VdUI+6F6c/p/A3wPeB3wfOHfGNpcCD9L9HMQm4PFRz3sJev6nwGlt+ZPLued++u3Z7hHgu8AVo573Evw3/iDwDPBr7fmZo573EvT8ReDGtvxh4DXgfaOe+yL7/hfArwNPH2X90P9+nchHCP18HcZlwF3V9RjwwSRrlnqiQzRvz1X1V1X1env6GN3PfCxX/X7lyb8DvgW8spSTO0b66flfAd+uqhcAqmq5991PzwX8apIAH6AbCNNLO83hqqrv0e3jaIb+9+tEDoS1wIs9zw+22kK3WU4W2s81dN9hLFfz9ptkLfAp4I+WcF7HUj//jf8BcFqSTpInk2xdstkdG/30/B+Bj9D9QOs+4AtV9culmd7IDP3v13HxOYRjpJ+vw+jrKzOWkb77SfKbdAPhnx3TGR1b/fT7H4Dfr6rD3TePy14/Pa8CLgIuBlYDjyZ5rKr+x7Ge3DHST8+XAE8BnwD+PvBQkv9eVT871pMboaH//TqRA6Gfr8M40b4yo69+kvxj4I+BT1bVT5dobsdCP/2OA/e2MDgDuDTJdFX9p6WZ4tD1++/61ar6OfDzJN8DPgos10Dop+ffBb5a3ZPrk0meB/4R8MTSTHEkhv7360Q+ZdTP12E8AGxtV+s3AW9U1aGlnugQzdtzkl8Dvg18Zhm/Yzxi3n6r6uyqWl9V64H7gX+7jMMA+vt3vQf450lWJfk7wG8Azy7xPIepn55foHtERJIxut+G/OMlneXSG/rfrxP2CKGO8nUYSf5NW/9HdO86uRSYBP433XcZy1afPf974HTgtvauebqW6TdF9tnvCaWfnqvq2SR/DvwA+CXwx1U1662Ly0Gf/52/AtyZZB/dUym/X1XL+iuxk3wTmADOSHIQ+BLwXjh2f7/86gpJEnBinzKSJC2AgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDX/F5XyJrirGgnkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub['toxic'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
